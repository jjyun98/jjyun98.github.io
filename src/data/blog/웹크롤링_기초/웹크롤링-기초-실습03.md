---
author: Jo YunHo
pubDatetime: 2025-02-01T07:22:00Z
modDatetime: 2025-02-01T10:12:00Z
title: 웹 크롤링 기초 - 정적 페이지 크롤링
slug: "webcrawling-3-basic"
featured: false
draft: false
tags:
  - Web Crawling
  - Python
  - BeautifulSoup
  - requests
  - html
  - css
description: "Python requests와 BeautifulSoup을 활용한 웹 크롤링 기초 정리"
---

> 📌 참고 영상:  
> [웹크롤링 비법 대공개! 데이터 수집 기초부터 돈버는 전략까지 한번에 배우기!](https://www.youtube.com/watch?v=Uf21RUo3KNc&list=PLNO7MWpu0eeUFdGMirV8_EkiLETqj8xA4)

## Table of contents

## 1. 정적 페이지 vs 동적 페이지

웹 페이지는 크게 **정적 페이지(static page)** 와 **동적 페이지(dynamic page)** 로 나눌 수 있다.

| 구분  | 특징 |
|------|------|
| **정적 페이지** | HTML이 고정되어 있으며, 서버에서 전달된 데이터가 그대로 표시됨 |
| **동적 페이지** | JavaScript 등의 기술로 서버와 비동기 통신을 하며 데이터가 실시간으로 변경됨 |

이번 글에서는 **정적 페이지 크롤링** 방법을 다룹니다.

## 2. URL 구조 이해하기

웹 크롤링을 하려면 **URL(Uniform Resource Locator)** 구조를 이해하는 것이 중요하다.
URL은 크게 **프로토콜(Protocol), 도메인(Domain), 경로(Path), 파라미터(Parameter)** 로 구성됩니다.

#### ✅ URL 예시

```python
https://search.naver.com/search.naver?where=news&query=삼성전자
```

|요소|설명|
|------|-----|
|**프로토콜 (Protocol)** | 통신 방식 지정 (https:)**//앞**|
|**도메인 (Domain)** | 웹사이트의 주소 (search.naver.com)**/앞**|
|**경로 (Path)** | 특정 페이지 위치 (/search.naver)**?앞**|
|**파라미터 (Parameter)** | 추가 정보를 서버에 전달 (?where=news&query=삼성전자)**?뒤**|

#### ✅ 파라미터 구조

URL에서 ? 뒤의 값이 **파라미터(Parameter)** 입니다.
여러 개의 파라미터는 &로 연결되며, 각 파라미터는 key=value 형태로 구성됩니다.
- where=news → where라는 키의 값이 news
- query=삼성전자 → query라는 키의 값이 삼성전자

📌 크롤링할 때 **URL에서 파라미터 부분을 변경하여 원하는 데이터를 가져올 수 있다.**

## 3. requests를 활용한 데이터 가져오기

Python의 `requests` 라이브러리를 사용하면 웹사이트에 요청을 보내고 HTML 데이터를 받아올 수 있습니다.

#### ✅ 기본 사용법

```python
import requests

response = requests.get("https://startcoding.pythonanywhere.com/basic")
print(response.status_code)  # 응답 코드 출력 (200이면 성공)
print(response.text)  # HTML 데이터 출력
```

만약 존재하지 않는 페이지에 접근하면 **404**라고 출력한다.

## 4. BeautifulSoup을 이용한 데이터 추출

웹 페이지에서 필요한 정보를 추출하기 위해 `BeautifulSoup` 라이브러리를 사용한다.

#### ✅ 기본 사용법


```python
from bs4 import BeautifulSoup

html = "<div class='brand-name'><a href='https://example.com'>브랜드</a></div>"
soup = BeautifulSoup(html, 'html.parser')

# 특정 요소 찾기
brand = soup.select_one(".brand-name")
print(brand.text)  # 브랜드

# 속성 값 가져오기
print(brand.select_one("a").attrs["href"])  # https://example.com
```

👉 CSS 선택자를 활용하여 원하는 데이터를 쉽게 추출할 수 있다. 선택자를 잘 사용하는 것이 핵심!

## 5. 실전 크롤링 예제

#### ✅ 상품 정보 크롤링

`requests`와 `BeautifulSoup`을 활용하여 상품 정보를 가져오는 예제입니다.

```python
import requests
from bs4 import BeautifulSoup

response = requests.get("https://startcoding.pythonanywhere.com/basic")
html = response.text
soup = BeautifulSoup(html, 'html.parser')

items = soup.select(".product")
for item in items:
    category = item.select_one(".product-category").text.strip()
    name = item.select_one(".product-name").text.strip()
    link = item.select_one(".product-name > a").attrs["href"]
    price = item.select_one(".product-price").text.split('원')[0].replace(',', '')

    print(f"카테고리: {category}, 상품명: {name}, 가격: {price}원, 링크: {link}")
```

👉 .product-category, .product-name, .product-price 같은 CSS 클래스를 활용하여 데이터를 추출한다.

## 6. 여러 페이지 크롤링

크롤링 대상 페이지가 여러 개일 경우, **반복문을 이용하여 자동화**할 수 있다.

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd

data = []
for i in range(1, 5):  # 1~4페이지 크롤링
    response = requests.get(f"https://startcoding.pythonanywhere.com/basic?page={i}")
    html = response.text
    soup = BeautifulSoup(html, 'html.parser')

    items = soup.select(".product")
    for item in items:
        category = item.select_one(".product-category").text.strip()
        name = item.select_one(".product-name").text.strip()
        link = item.select_one(".product-name > a").attrs["href"]
        price = item.select_one(".product-price").text.split('원')[0].replace(',', '')

        data.append({
            "Category": category,
            "Name": name,
            "Link": link,
            "Price": price
        })

df = pd.DataFrame(data)
print(df)
```

👉 ?page=숫자 형태로 URL이 변하는 부분을 활용하여 자동으로 여러 페이지를 가져옵니다.

## 마무리

이번 글에서는 **requests와 BeautifulSoup을 이용한 웹 크롤링 기초**를 다뤘습니다.
다음 단계에서는 **동적 페이지 크롤링과 Selenium 자동화**에 대해서도 다뤄보겠습니다. 🚀